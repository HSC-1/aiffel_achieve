{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (E6) 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Now greetings to the world! Standing at this liquor store,', 'Whiskey coming through my pores,', 'Feeling like I run this whole block.']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np     \n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now greetings to the world ! standing at this liquor store , <end>',\n",
       " '<start> whiskey coming through my pores , <end>',\n",
       " '<start> feeling like i run this whole block . <end>',\n",
       " '<start> lotto tickets cheap beer <end>',\n",
       " '<start> that s why you can catch me here , <end>',\n",
       " '<start> tryna scratch my way up to the top . cause my job got me going nowhere , <end>',\n",
       " '<start> so i ain t got a thing to lose . <end>',\n",
       " '<start> take me to a place where i don t care , <end>',\n",
       " '<start> this is me and my liquor store blues . i ll take one shot for my pain , <end>',\n",
       " '<start> one drag for my sorrow . <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50 5041 ...    4    3    0]\n",
      " [   2 2757  286 ...    0    0    0]\n",
      " [   2  331   25 ...    0    0    0]\n",
      " ...\n",
      " [   2    5   33 ...    0    0    0]\n",
      " [   2    5   33 ...   13  169    3]\n",
      " [   2    5  373 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0f0b8356d0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    a =[]\n",
    "    for i in tensor:    \n",
    "        if len(i) < 16:\n",
    "            a.append(i)\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(a, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50 5041   10    6  143   59  581   71   42 1372 1043    4    3]\n",
      "[  50 5041   10    6  143   59  581   71   42 1372 1043    4    3    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "# len(tokenizer.index_word)\n",
    "vocab_size = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "embedding_size = 512\n",
    "hidden_size = 2024\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,embedding_size, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size,return_sequences = True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size,return_sequences = True))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 512)         6144512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 2024)        20539552  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 2024)        32780704  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 12001)       24302025  \n",
      "=================================================================\n",
      "Total params: 83,766,793\n",
      "Trainable params: 83,766,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 190s 390ms/step - loss: 3.3221 - val_loss: 2.9553\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 191s 392ms/step - loss: 2.7857 - val_loss: 2.7017\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 192s 393ms/step - loss: 2.4946 - val_loss: 2.5287\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 193s 396ms/step - loss: 2.2118 - val_loss: 2.3879\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 193s 396ms/step - loss: 1.9360 - val_loss: 2.2814\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 193s 396ms/step - loss: 1.6804 - val_loss: 2.2033\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 193s 396ms/step - loss: 1.4589 - val_loss: 2.1567\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 193s 396ms/step - loss: 1.2797 - val_loss: 2.1371\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 193s 397ms/step - loss: 1.1464 - val_loss: 2.1436\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 195s 401ms/step - loss: 1.0591 - val_loss: 2.1603\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "model.compile(loss = loss,optimizer=optimizer)\n",
    "history = model.fit(train_dataset,validation_data = val_dataset,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, 11)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl , <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 작업 \n",
    "만드는 문장 길이가 짧아서 길게 만들고 싶어 generate_text를 수정해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_up(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    a=0\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        \n",
    "       \n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token:\n",
    "            a += 1\n",
    "            if  a == 2:break\n",
    "            \n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "    generated = \"\"\n",
    "#     predict_word.numpy().shape\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        if word_index == 0:\n",
    "            generated += \"더 이상 못만들겠소\"\n",
    "            break\n",
    "        else:\n",
    "            generated += tokenizer.index_word[word_index] + \" \"\n",
    "#     generated = []\n",
    "#     for word_index in test_tensor[0].numpy():\n",
    "#         generated.append(word_index)\n",
    "\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl , <end> 더 이상 못만들겠소'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_up(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해본 결과 이미 END가 나온 상태에서 더 진행을 해도 아무 값이 없는 상태로 예측을 못하는것으로 파악하였다. 그럴 경우 더이상 못만들겠다고 말하도록하고 다른 단어는 더 만들수 있나 찾아보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> do you know what you re kicking away ? <end> 더 이상 못만들겠소\n",
      "<start> i hate the new kanye , the bad mood kanye <end> 더 이상 못만들겠소\n",
      "<start> love is a feeling <end> 더 이상 못만들겠소\n",
      "<start> i hate you <unk> you know that i love you boy <end> 더 이상 못만들겠소\n",
      "<start> why why they mad they aint famous god damn <end> 더 이상 못만들겠소\n"
     ]
    }
   ],
   "source": [
    "a = generate_text_up(model, tokenizer, init_sentence=\"<start> do you know\")\n",
    "print(a)\n",
    "a = generate_text_up(model, tokenizer, init_sentence=\"<start> i hate\")\n",
    "print(a)\n",
    "a = generate_text_up(model, tokenizer, init_sentence=\"<start> love\")\n",
    "print(a)\n",
    "a = generate_text_up(model, tokenizer, init_sentence=\"<start> i hate you very, you know\")\n",
    "print(a)\n",
    "a = generate_text_up(model, tokenizer, init_sentence=\"<start> why why\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "못찾았다.... 한번 END가 나온 상태면 모델이 예측을 안하는 것 같다. \n",
    "\n",
    "---\n",
    "## 고찰\n",
    "작사를 시인처럼 하고파서 제목도 poet라고 지었지만 아주 짧은 문장만 만들 수 있는 모델이 되어 아쉬웠다.  \n",
    "데이터만 있다면 시조도 만들 수 있는 자연어 처리 모델을 한번 만들고 싶다.\n",
    "벨리데이션 로스 값 2.2 이하인 2.16까지 맞추어 주었다. 임베딩 사이즈 보다 히든 레이어에 변화에 더 좋은 결과가 나왔다. 임베딩 사이즈는 어느정도 이상이 의미가 퇴색이 되어 별 의미가 없어졌다. \n",
    "\n",
    "\n",
    "---\n",
    "## 결론\n",
    "작사를 시인처럼 하고파서 제목도 poet라고 지었지만 아주 짧은 문장만 만들 수 있는 모델이 되어 아쉬웠다.  \n",
    "데이터만 있다면 시조도 만들 수 있는 자연어 처리 모델을 한번 만들고 싶다. \n",
    "\n",
    "---\n",
    "### 이번의 변화\n",
    "다양한 시도를 해보았다. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
